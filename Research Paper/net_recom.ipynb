{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ad72abc6-6c95-45b2-aacc-42fbb85b8507",
    "_uuid": "4755c4915c1449fc9d5be0e7d3cba71a3bbcce09"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "# Netflix Recommender System\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e7fd6bbb-9fb6-455f-8f14-6fb25d55c866",
    "_uuid": "8482ce5398e66af9faa64f603c4ebd2b5324ad33"
   },
   "source": [
    "## Table of Content:\n",
    "\n",
    "* Objective\n",
    "\n",
    "* Data manipulation\n",
    "    -  Data loading\n",
    "    -  Data viewing\n",
    "    -  Data cleaning\n",
    "    -  Data slicing\n",
    "    -  Data mapping\n",
    "    \n",
    "* Recommendation models\n",
    "    -  Recommend with Collaborative Filtering \n",
    "    -  Recommend with Pearsons' R correlation\n",
    "    -  Recommend with Probabilistic matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68066366-4219-4779-a159-d503bdedbfdd",
    "_uuid": "689e0500abe3bd15f72c1cb3010c538c90631b50"
   },
   "source": [
    "# Objective\n",
    "<br>\n",
    "Learn from data and recommend best TV shows to users, based on self & others behaviour\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fbfaefbf-fda2-46ce-9e22-59b2c8d17fa2",
    "_uuid": "8656955e16b88d57f19a3789c90069059ba884b9"
   },
   "source": [
    "# Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a824059-8c9b-4418-9a24-f833943d49cb",
    "_uuid": "2101d86c2cf3f7a61475ea82aaa3c5dd068cf187"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4092e050-1938-4283-9b18-396c60e94ee1",
    "_uuid": "d0978db1b40af98cf11b5b185ef264a9891d183d"
   },
   "source": [
    "Each data file (there are 4 of them) contains below columns:\n",
    "\n",
    "* Movie ID (as first line of each new movie record / file)\n",
    "* Customer ID\n",
    "* Rating (1 to 5)\n",
    "* Date they gave the ratings\n",
    "\n",
    "There is another file contains the mapping of Movie ID to the movie background like name, year of release, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "637b34e2-b123-4b2d-8e70-97631b0321f9",
    "_uuid": "1f60257741c703435318df7e05e2a46c6e11af63"
   },
   "source": [
    "Let's import the library we needed before we get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "046298b9-7ef7-4e68-aef2-a1fe316be5a0",
    "_uuid": "3bc39967a41f9ec3989f971c49916b822b0806b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Reader, Dataset, SVD, evaluate\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be4477f1-7a11-48f4-8147-262a6198609f",
    "_uuid": "665b9a4bceca7bb318e39f1a5825170b18c6cc63"
   },
   "source": [
    "Next let's load first data file and get a feeling of how huge the dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "0343ba37-0654-469c-98e5-812ecbaca528",
    "_uuid": "2a5476e11ee4539c129f2da35fccdacf2c296765",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 shape: (24058263, 2)\n",
      "-Dataset examples-\n",
      "          Cust_Id  Rating\n",
      "0              1:     NaN\n",
      "5000000   2560324     4.0\n",
      "10000000  2271935     2.0\n",
      "15000000  1921803     2.0\n",
      "20000000  1933327     3.0\n"
     ]
    }
   ],
   "source": [
    "# Skip date\n",
    "df1 = pd.read_csv('./combined_data_1.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n",
    "\n",
    "df1['Rating'] = df1['Rating'].astype(float)\n",
    "\n",
    "print('Dataset 1 shape: {}'.format(df1.shape))\n",
    "print('-Dataset examples-')\n",
    "print(df1.iloc[::5000000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5d0ced5-5376-4ff5-86f9-e642a7adbd92",
    "_uuid": "3509640b273342e38c2635d1bb003e0d33de9e8c"
   },
   "source": [
    "Let's try to load the 3 remaining dataset as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a093a49-8a80-4afd-bc13-17b84b284142",
    "_uuid": "a6ca9915b92abd2681ae9a355d446e73b6fbe795",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv('../input/combined_data_2.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n",
    "#df3 = pd.read_csv('../input/combined_data_3.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n",
    "#df4 = pd.read_csv('../input/combined_data_4.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n",
    "\n",
    "\n",
    "#df2['Rating'] = df2['Rating'].astype(float)\n",
    "#df3['Rating'] = df3['Rating'].astype(float)\n",
    "#df4['Rating'] = df4['Rating'].astype(float)\n",
    "\n",
    "#print('Dataset 2 shape: {}'.format(df2.shape))\n",
    "#print('Dataset 3 shape: {}'.format(df3.shape))\n",
    "#print('Dataset 4 shape: {}'.format(df4.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11ca529c-e11c-4ec1-b9e9-d6c6c45163de",
    "_uuid": "ebf5b154314c1268b4fffdf0449172b71e393c4f"
   },
   "source": [
    "Now we combine datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ded88177-b586-48f2-bf3d-e1a892aca10e",
    "_uuid": "4ea5a28d0108d2b272f1d30cf749080c4e94e66d",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (24058263, 2)\n",
      "-Dataset examples-\n",
      "          Cust_Id  Rating\n",
      "0              1:     NaN\n",
      "5000000   2560324     4.0\n",
      "10000000  2271935     2.0\n",
      "15000000  1921803     2.0\n",
      "20000000  1933327     3.0\n"
     ]
    }
   ],
   "source": [
    "# load less data for speed\n",
    "\n",
    "df = df1\n",
    "#df = df1.append(df2)\n",
    "#df = df.append(df3)\n",
    "#df = df.append(df4)\n",
    "\n",
    "df.index = np.arange(0,len(df))\n",
    "print('Full dataset shape: {}'.format(df.shape))\n",
    "print('-Dataset examples-')\n",
    "print(df.iloc[::5000000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78a857d7-1ab1-4d93-b750-9c14b4ba2c9a",
    "_uuid": "5bfa706c8f28f965b669dcfb285c9c32c1478bad"
   },
   "source": [
    "## Data viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "48f3f057-706a-4667-b58e-79d70893cbb1",
    "_uuid": "b96e6aebfe14e3be18722b759654b732b8fa4d51"
   },
   "source": [
    "Let's give a first look on how the data spread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0d82d7df-6c77-44f2-a0bc-70ae0324329f",
    "_uuid": "7e8780821d463af5bdcee9ec2662cf27d89745e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = df.groupby('Rating')['Rating'].agg(['count'])\n",
    "\n",
    "# get movie count\n",
    "movie_count = df.isnull().sum()[1]\n",
    "\n",
    "# get customer count\n",
    "cust_count = df['Cust_Id'].nunique() - movie_count\n",
    "\n",
    "# get rating count\n",
    "rating_count = df['Cust_Id'].count() - movie_count\n",
    "\n",
    "ax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\n",
    "plt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count), fontsize=20)\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(1,6):\n",
    "    ax.text(p.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 / p.sum()[0]), color = 'white', weight = 'bold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7394a2b-8c79-40b8-b967-765d3ae0ad10",
    "_uuid": "dddad55f2699f3f4c02ae64a3e470c314e248643"
   },
   "source": [
    "We can see that the rating tends to be relatively positive (>3). This may be due to the fact that unhappy customers tend to just leave instead of making efforts to rate. We can keep this in mind - low rating movies mean they are generally really bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "581427e0-87df-46b1-a0af-7eb06932b1a3",
    "_uuid": "bf7bd867b322b3e40c4eb1204d345029b4eb31b6"
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3165defc-df86-49a8-ba51-6abb9fa253b1",
    "_uuid": "f232d44b5a8282bdcfbab54861bbd7990132e2c7"
   },
   "source": [
    "Movie ID is really a mess import! Looping through dataframe to add Movie ID column WILL make the Kernel run out of memory as it is too inefficient. I achieve my task by first creating a numpy array with correct length then add the whole array as column into the main dataframe! Let's see how it is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "d06e0993-d5ff-4f75-87a7-7659f5427ebf",
    "_uuid": "498476341fad8d25d24090c07ea4b48299f9424a",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie numpy: [  1.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   4.49900000e+03\n",
      "   4.49900000e+03   4.49900000e+03]\n",
      "Length: 24053764\n"
     ]
    }
   ],
   "source": [
    "df_nan = pd.DataFrame(pd.isnull(df.Rating))\n",
    "df_nan = df_nan[df_nan['Rating'] == True]\n",
    "df_nan = df_nan.reset_index()\n",
    "\n",
    "movie_np = []\n",
    "movie_id = 1\n",
    "\n",
    "for i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n",
    "    # numpy approach\n",
    "    temp = np.full((1,i-j-1), movie_id)\n",
    "    movie_np = np.append(movie_np, temp)\n",
    "    movie_id += 1\n",
    "\n",
    "# Account for last record and corresponding length\n",
    "# numpy approach\n",
    "last_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\n",
    "movie_np = np.append(movie_np, last_record)\n",
    "\n",
    "print('Movie numpy: {}'.format(movie_np))\n",
    "print('Length: {}'.format(len(movie_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "e7da935d-a055-4ce6-9509-9c0439fda1de",
    "_uuid": "73c7888f9cf7e1d0f705c6a14019d9371eaa9bf3",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Dataset examples-\n",
      "          Cust_Id  Rating  Movie_Id\n",
      "1         1488844     3.0         1\n",
      "5000996    501954     2.0       996\n",
      "10001962   404654     5.0      1962\n",
      "15002876   886608     2.0      2876\n",
      "20003825  1193835     2.0      3825\n"
     ]
    }
   ],
   "source": [
    "# remove those Movie ID rows\n",
    "df = df[pd.notnull(df['Rating'])]\n",
    "\n",
    "df['Movie_Id'] = movie_np.astype(int)\n",
    "df['Cust_Id'] = df['Cust_Id'].astype(int)\n",
    "print('-Dataset examples-')\n",
    "print(df.iloc[::5000000, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fd1a2d66-78b0-4191-8ca2-0caef60e91fa",
    "_uuid": "7abf85f047576e1c8fe7742e28bd2a55d33c366c"
   },
   "source": [
    "## Data slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6532819a-7b08-45c4-8b25-952568d7d465",
    "_uuid": "b0107145609698c552ad9e74fd192cbbe93c4bb3"
   },
   "source": [
    "The data set now is super huge. I have tried many different ways but can't get the Kernel running as intended without memory error. Therefore I tried to reduce the data volumn by improving the data quality below:\n",
    "\n",
    "* Remove movie with too less reviews (they are relatively not popular)\n",
    "* Remove customer who give too less reviews (they are relatively less active)\n",
    "\n",
    "Having above benchmark will have significant improvement on efficiency, since those unpopular movies and non-active customers still occupy same volumn as those popular movies and active customers in the view of matrix (NaN still occupy space). This should help improve the statistical signifiance too.\n",
    "\n",
    "Let's see how it is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1db45c46-ee82-4db5-be2c-919258c09d47",
    "_uuid": "b8987bf7e2cfcdc2a69fb767c4033d05240cc5a3",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie minimum times of review: 3884.0\n",
      "Customer minimum times of review: 79.0\n"
     ]
    }
   ],
   "source": [
    "f = ['count','mean']\n",
    "\n",
    "df_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)\n",
    "df_movie_summary.index = df_movie_summary.index.map(int)\n",
    "movie_benchmark = round(df_movie_summary['count'].quantile(0.8),0)\n",
    "drop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n",
    "\n",
    "print('Movie minimum times of review: {}'.format(movie_benchmark))\n",
    "\n",
    "df_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)\n",
    "df_cust_summary.index = df_cust_summary.index.map(int)\n",
    "cust_benchmark = round(df_cust_summary['count'].quantile(0.8),0)\n",
    "drop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n",
    "\n",
    "print('Customer minimum times of review: {}'.format(cust_benchmark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bebeaf19-b3a0-45d9-8a91-deaff2881d71",
    "_uuid": "bc6022b8d87bfb7679984bcbd4b928a54ef19be8"
   },
   "source": [
    "Now let's trim down our data, whats the difference in data size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "61f85e6a-3438-456b-b169-f42c0270a752",
    "_uuid": "f09c53f0e7b7fea039437c43e5163a5a59250b70",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: (24053764, 3)\n",
      "After Trim Shape: (13528427, 3)\n",
      "-Data Examples-\n",
      "          Cust_Id  Rating  Movie_Id\n",
      "5109       785314     1.0         8\n",
      "8889698    332300     3.0      1770\n",
      "17751978   629874     4.0      3391\n"
     ]
    }
   ],
   "source": [
    "print('Original Shape: {}'.format(df.shape))\n",
    "df = df[~df['Movie_Id'].isin(drop_movie_list)]\n",
    "df = df[~df['Cust_Id'].isin(drop_cust_list)]\n",
    "print('After Trim Shape: {}'.format(df.shape))\n",
    "print('-Data Examples-')\n",
    "print(df.iloc[::5000000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f10cc54-4021-4748-9f2f-933d541acee4",
    "_uuid": "ea0da55846a3795aead5d0365d5fcf91b03636ab"
   },
   "source": [
    "Let's pivot the data set and put it into a giant matrix - we need it for our recommendation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9e5a21fd-ccff-4fd3-aebe-cd82e5734ba9",
    "_uuid": "528c8ecb8bbd94130e38e68362184087dcc39f83",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95325, 900)\n"
     ]
    }
   ],
   "source": [
    "df_p = pd.pivot_table(df,values='Rating',index='Cust_Id',columns='Movie_Id')\n",
    "\n",
    "print(df_p.shape)\n",
    "\n",
    "# Below is another way I used to sparse the dataframe...doesn't seem to work better\n",
    "\n",
    "#Cust_Id_u = list(sorted(df['Cust_Id'].unique()))\n",
    "#Movie_Id_u = list(sorted(df['Movie_Id'].unique()))\n",
    "#data = df['Rating'].tolist()\n",
    "#row = df['Cust_Id'].astype('category', categories=Cust_Id_u).cat.codes\n",
    "#col = df['Movie_Id'].astype('category', categories=Movie_Id_u).cat.codes\n",
    "#sparse_matrix = csr_matrix((data, (row, col)), shape=(len(Cust_Id_u), len(Movie_Id_u)))\n",
    "#df_p = pd.DataFrame(sparse_matrix.todense(), index=Cust_Id_u, columns=Movie_Id_u)\n",
    "#df_p = df_p.replace(0, np.NaN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62ba3943-5369-4df9-b33e-3a5a8b47e9f9",
    "_uuid": "598d5c85304513168b871257f5c3bd810a7a8da4"
   },
   "source": [
    "## Data mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7768ffa2-e387-4b15-8ef1-c808229f4dc0",
    "_uuid": "ab718ba4d0e6b3b95d03c25b577884e88af77b93"
   },
   "source": [
    "Now we load the movie mapping file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "cec6d42b-adff-49c0-939c-2f92adae15a4",
    "_uuid": "d971e5a1ccd038f9a08e126daeb8995d30f9e014",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Year                          Name\n",
      "Movie_Id                                      \n",
      "1         2003.0               Dinosaur Planet\n",
      "2         2004.0    Isle of Man TT 2004 Review\n",
      "3         1997.0                     Character\n",
      "4         1994.0  Paula Abdul's Get Up & Dance\n",
      "5         2004.0      The Rise and Fall of ECW\n",
      "6         1997.0                          Sick\n",
      "7         1992.0                         8 Man\n",
      "8         2004.0    What the #$*! Do We Know!?\n",
      "9         1991.0      Class of Nuke 'Em High 2\n",
      "10        2001.0                       Fighter\n"
     ]
    }
   ],
   "source": [
    "df_title = pd.read_csv('./movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\n",
    "df_title.set_index('Movie_Id', inplace = True)\n",
    "print (df_title.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1b464a7-3c69-4acb-adcb-00d8b3ec9c93",
    "_uuid": "c6becd707a3c14a5c76887789e21e9fbf150e9f8"
   },
   "source": [
    "# Recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a73d197f-1700-40ae-b20c-cab9b8e0c008",
    "_uuid": "bd73e13a984d412908272360c701c15b58f412df"
   },
   "source": [
    "Well all data required is loaded and cleaned! Next let's get into the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa6ad634-5c47-41e1-adeb-47fe2bd8f1b9",
    "_uuid": "523277beb220f90b2f7fb58dab680e22db2aa325"
   },
   "source": [
    "## Recommend with Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalute performance of [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering), with just first 100K rows for faster process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "fa414c7c-f908-40fa-bc99-9b221748c923",
    "_uuid": "a59aaac88ca121b93fcd8807e462dc7f0b609254",
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9864\n",
      "MAE:  0.8054\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9885\n",
      "MAE:  0.8020\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9840\n",
      "MAE:  0.7935\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9863\n",
      "Mean MAE : 0.8003\n",
      "------------\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CaseInsensitiveDefaultDict(list,\n",
       "                           {'mae': [0.80540555988387574,\n",
       "                             0.80199688806555047,\n",
       "                             0.79349126236113532],\n",
       "                            'rmse': [0.98638274169327778,\n",
       "                             0.98848899187831596,\n",
       "                             0.98402192661248977]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader()\n",
    "\n",
    "# get just top 100K rows for faster run time\n",
    "data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']][:100000], reader)\n",
    "data.split(n_folds=3)\n",
    "\n",
    "svd = SVD()\n",
    "evaluate(svd, data, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check user 683421 preference in the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_683421 = df[(df['Cust_Id'] == 683421) & (df['Rating'] == 5)]\n",
    "df_683421 = df_683421.set_index('Movie_Id')\n",
    "df_683421 = df_683421.join(df_title)['Name']\n",
    "print(df_683421)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict which movies user 683421 would love to watch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "7da1d4f5-ef96-4f33-96ae-a66028f8926d",
    "_uuid": "6f47935958ad57c568fa58253a83452abe83fbed",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Year                                   Name  Estimate_Score\n",
      "3045  1990.0      The Simpsons: Treehouse of Horror        5.000000\n",
      "2171  1991.0                 The Simpsons: Season 3        5.000000\n",
      "2101  1994.0                 The Simpsons: Season 6        4.934957\n",
      "2056  2001.0     Buffy the Vampire Slayer: Season 6        4.903174\n",
      "1551  1983.0                            Black Adder        4.866410\n",
      "3443  2004.0  Family Guy: Freakin' Sweet Collection        4.866346\n",
      "2113  2002.0                                Firefly        4.817488\n",
      "4114  1999.0                The Simpsons: Bart Wars        4.807122\n",
      "3167  1987.0              Evil Dead 2: Dead by Dawn        4.794396\n",
      "4391  1993.0                       Army of Darkness        4.770021\n"
     ]
    }
   ],
   "source": [
    "user_683421 = df_title.copy()\n",
    "user_683421 = user_683421.reset_index()\n",
    "user_683421 = user_683421[~user_683421['Movie_Id'].isin(drop_movie_list)]\n",
    "\n",
    "# getting full dataset\n",
    "data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']], reader)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "svd.train(trainset)\n",
    "\n",
    "user_683421['Estimate_Score'] = user_683421['Movie_Id'].apply(lambda x: svd.predict(683421, x).est)\n",
    "\n",
    "user_683421 = user_683421.drop('Movie_Id', axis = 1)\n",
    "\n",
    "user_683421 = user_683421.sort_values('Estimate_Score', ascending=False)\n",
    "print(user_683421.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25d88f52-36a2-4c62-bdc5-fee174b1aab7",
    "_uuid": "f24caa335865e13f5e0feb47bfb0b47996c69570"
   },
   "source": [
    "## Recommend with Pearsons' R correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1220239-a049-4543-9ab6-89e1d00d6cdd",
    "_uuid": "4d4082f49c67f7406af44d9125ddab4c16ca967e"
   },
   "source": [
    "The way it works is we use Pearsons' R correlation to measure the linear correlation between review scores of all pairs of movies, then we provide the top 10 movies with highest correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "200cf4e8-59d6-459d-a0e5-5c9452bc8ad0",
    "_uuid": "531e21998a34956e35f3e0a839e18d528faa6709",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend(movie_title, min_count):\n",
    "    print(\"For movie ({})\".format(movie_title))\n",
    "    print(\"- Top 10 movies recommended based on Pearsons'R correlation - \")\n",
    "    i = int(df_title.index[df_title['Name'] == movie_title][0])\n",
    "    target = df_p[i]\n",
    "    similar_to_target = df_p.corrwith(target)\n",
    "    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])\n",
    "    corr_target.dropna(inplace = True)\n",
    "    corr_target = corr_target.sort_values('PearsonR', ascending = False)\n",
    "    corr_target.index = corr_target.index.map(int)\n",
    "    corr_target = corr_target.join(df_title).join(df_movie_summary)[['PearsonR', 'Name', 'count', 'mean']]\n",
    "    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c06b5afa-cf47-4853-a712-ee0afe60b994",
    "_uuid": "e7b9374a3c6bcd9d890f1a1462d6d4f2cb55dad5"
   },
   "source": [
    "A recommendation for you if you like 'What the #$*! Do We Know!?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "9691bc55-4bde-4580-ae43-f9698e46ab81",
    "_uuid": "903cb1f6529d9d93deb557b5ac7eeba4b42d8a53",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For movie (What the #$*! Do We Know!?)\n",
      "- Top 10 movies recommended based on Pearsons'R correlation - \n",
      "PearsonR                                      Name  count      mean\n",
      "1.000000                What the #$*! Do We Know!?  14910  3.189805\n",
      "0.326285                          The 10th Kingdom   4532  3.669903\n",
      "0.324225              Star Trek: Voyager: Season 5   4175  4.111377\n",
      "0.316818                                  Together   3958  3.889843\n",
      "0.312717      Star Trek: Deep Space Nine: Season 5   4373  4.160759\n",
      "0.309444           To Gillian on her 37th Birthday   4261  3.102793\n",
      "0.289334              Murder on the Orient Express   4785  3.743783\n",
      "0.288829        Nausicaa of the Valley of the Wind   6674  4.189542\n",
      "0.288389  Star Trek: The Next Generation: Season 5   8978  4.247494\n",
      "0.286320         The Life & Death of Peter Sellers   4702  3.099957\n"
     ]
    }
   ],
   "source": [
    "recommend(\"What the #$*! Do We Know!?\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c4f1cfbe-0c57-46b5-97d4-1f9466862802",
    "_uuid": "3646da79d856c2895001832bedb3bf5236a84166"
   },
   "source": [
    "X2: X-Men United:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "9fc24be5-1037-4208-b1ca-07e2e9f8c4a7",
    "_uuid": "8586765d4b658bea95997f4418b7ae14c2c6be3d",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For movie (X2: X-Men United)\n",
      "- Top 10 movies recommended based on Pearsons'R correlation - \n",
      "PearsonR                      Name   count      mean\n",
      "1.000000          X2: X-Men United   98720  3.932202\n",
      "0.342535             Batman Begins   54922  4.236699\n",
      "0.313025  Mortal Kombat: The Movie    7633  3.165466\n",
      "0.311354      The Matrix: Reloaded  106807  3.514704\n",
      "0.303351    Todd McFarlane's Spawn    4684  3.612511\n",
      "0.295555   The Matrix: Revolutions   60415  3.517040\n",
      "0.295549                Species II    7526  2.781424\n",
      "0.294434                 Daredevil   62628  2.979642\n",
      "0.293918      Smallville: Season 1   10082  4.169609\n",
      "0.293820            Blade: Trinity   42727  3.548974\n"
     ]
    }
   ],
   "source": [
    "recommend(\"X2: X-Men United\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9ba141d-cec5-4104-935e-f0492ce099df",
    "_uuid": "08abd9eabd6c0999751f5cab879e9976517e61d8"
   },
   "source": [
    "Hope it sounds good now.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
